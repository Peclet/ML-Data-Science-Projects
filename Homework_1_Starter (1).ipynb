{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Homework 1 Starter",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "DZPI0mQAiOhJ",
        "fist87zMfw1S",
        "NlmQQQcWIjBE"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8KOkEolWlki",
        "colab_type": "text"
      },
      "source": [
        "# Homework 1\n",
        "\n",
        "\n",
        "In this assignment, we will explore some basic machine learning algorithms to solve classification problems:\n",
        "\n",
        "\n",
        "*   k-Nearest Neighbors\n",
        "*   Decision Trees\n",
        "*   Random Forest\n",
        "\n",
        "For full credit, you will need to create visualizations demonstrating each of these. \n",
        "\n",
        "Additionally, please review the conclusions at the bottom.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5ay9P7TKNhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import libraries:\n",
        "\n",
        "# NumPy is an efficient multidimensional vector/matrix manipulation library. \n",
        "# All the libraries we use will be designed to work with NumPy, so it is \n",
        "# incredibly beneficial to learn how to use this. \n",
        "# Documentation: https://docs.scipy.org/doc/numpy/reference/\n",
        "import numpy as np\n",
        "\n",
        "# pandas is a powerful way to store, manage, and maniupate tabular data. \n",
        "# Documentation: https://pandas.pydata.org/pandas-docs/stable/\n",
        "import pandas as pd\n",
        "\n",
        "# matplotlib is a plotting tool\n",
        "# Documentation: https://matplotlib.org/3.1.1/contents.html\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scikit-learn is a library in Python that provides many unsupervised and supervised learning algorithms\n",
        "# Documentation: https://scikit-learn.org/stable/documentation.html\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZPI0mQAiOhJ",
        "colab_type": "text"
      },
      "source": [
        "## Pre-made code:\n",
        "These are some functions to help us out along the way. Don't worry, they've already been completed. Feel free to read through this, otherwise skip ahead to \"Method 1: k-Nearest Neighbors\" to get started. **Make sure you run these code blocks first!!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fist87zMfw1S",
        "colab_type": "text"
      },
      "source": [
        "### Visualization Function\n",
        "This code will be used to visualize our results, no work needs to be done here for this assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjauHVzMf1_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_classifier(data, clf, points_on=True):\n",
        "    #   # create the domain for the plot\n",
        "    x1_min = data.x1.min()\n",
        "    x1_max = data.x1.max()\n",
        "    x2_min = data.x2.min()\n",
        "    x2_max = data.x2.max()\n",
        "\n",
        "    x1 = np.linspace(x1_min, x1_max, 200)\n",
        "    x2 = np.linspace(x2_min, x2_max, 200)\n",
        "    X1,X2 = np.meshgrid(x1, x2)\n",
        "\n",
        "    # convert it into a matrix (rows are locations, columns are features)\n",
        "    vis_data = np.hstack([X1.reshape(-1,1),X2.reshape(-1,1)])\n",
        "\n",
        "    # Get classifications for each location\n",
        "    vis_sco = clf.predict_proba(vis_data)\n",
        "    vis_class = clf.predict(vis_data)\n",
        "\n",
        "  # Get classifications for the test points:\n",
        "    X = np.stack((data.x1.to_numpy(), data.x2.to_numpy()), axis=1)\n",
        "    Y = data.y.to_numpy()\n",
        "    labels_test = clf.predict(X)\n",
        "    compare = np.stack((labels_test, Y), axis=1)\n",
        "    count = 0\n",
        "    for i in compare:\n",
        "        if i[0] == i[1]:\n",
        "            count += 1\n",
        "    total = len(compare)\n",
        "    accuracy = (count/total)\n",
        "\n",
        "    vis_sco = np.delete(vis_sco, 0, axis=1)\n",
        "\n",
        "\n",
        "    # convert back into image shapes\n",
        "    vis_classZ = vis_class.reshape(X1.shape)\n",
        "    vis_sco = vis_sco.reshape(X1.shape)\n",
        "\n",
        "    #\n",
        "    # Make the plots\n",
        "    #\n",
        "\n",
        "    # show the function value in the background\n",
        "    cs = plt.imshow(vis_sco,\n",
        "        extent=(x1_min,x1_max,x2_max,x2_min), # define limits of grid, note reversed y axis\n",
        "        cmap=plt.cm.jet, vmin=0.,vmax=1.)\n",
        "    plt.clim(0,1) # defines the value to assign the min/max color\n",
        "\n",
        "    # draw the line on top\n",
        "    levels = np.array([.5])\n",
        "    cs_line = plt.contour(X1,X2,vis_sco,levels, colors='k')\n",
        "    \n",
        "    if points_on:\n",
        "        plt.scatter(data.x1,data.x2,c=data.y,edgecolors='w',cmap=plt.get_cmap('jet'))\n",
        "\n",
        "    # add a color bar\n",
        "    CB = plt.colorbar(cs)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # print accuarcy below chart:\n",
        "    print(\"Test Accuracy: %s%%\" % (accuracy*100))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlmQQQcWIjBE",
        "colab_type": "text"
      },
      "source": [
        "### Make Data\n",
        "\n",
        "This function creates a synthetic dataset from a mean and covariance. Below, a set of synthetic datasets are generated which will be used for this assignment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRNrYERgOz5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_from_mean_and_cov(means, covs, labels, mode='train', count=100):\n",
        "  \n",
        "  np.random.seed(0) if mode == 'train' else np.random.seed(1)\n",
        "      \n",
        "  vals = np.array([]).reshape(0,len(means[0])+1)\n",
        "\n",
        "  for i, (mean,cov,label) in enumerate(zip(means,covs,labels)):\n",
        "    vals_new = np.random.multivariate_normal(mean,cov,count);\n",
        "    vals_new = np.hstack([vals_new,np.ones((vals_new.shape[0],1))*label])\n",
        "    vals = np.vstack([vals,vals_new])    \n",
        "\n",
        "  df = pd.DataFrame(data=vals,columns=['x1','x2','y'])\n",
        "\n",
        "  return df\n",
        "\n",
        "# Generate six training and testing datasets\n",
        "\n",
        "datasets = {\n",
        "    'train1':gen_from_mean_and_cov([[10,1],[0,4]],[[[2, 0],[0, 2]],[[2, 0],[0, 2]]],[1,0]),\n",
        "    'train2':gen_from_mean_and_cov([[10,1],[0,4]],[[[2, 0],[0, 2]],[[2, 0],[0, 2]]],[0,1]),\n",
        "    'train3':gen_from_mean_and_cov([[0,1],[0,3.5]],[[[1, .8],[.8, 1]],[[1, .8],[.8, 1]]],[1,0]),\n",
        "    'train4':gen_from_mean_and_cov([[1,0],[3,1]],[[[1, .8],[.8, 1]],[[1, .9],[.9, 1]]],[1,0]),\n",
        "    'train5':gen_from_mean_and_cov([[0,0],[4,0],[-4,0],[0,4],[0,-4]],[[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]]],[1,0,0,0,0]),\n",
        "    'train6':gen_from_mean_and_cov(\n",
        "    [[0,0],[4,0],[-4,0],[0,4],[0,-4],[-4,4],[4,4],[4,-4],[-4,-4]],\n",
        "    [[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]]],\n",
        "    [1,0,0,0,0,1,1,1,1]),\n",
        "    'test1':gen_from_mean_and_cov([[10,1],[0,4]],[[[2, 0],[0, 2]],[[2, 0],[0, 2]]],[1,0],mode='test'),\n",
        "    'test2':gen_from_mean_and_cov([[10,1],[0,4]],[[[2, 0],[0, 2]],[[2, 0],[0, 2]]],[0,1],mode='test'),\n",
        "    'test3':gen_from_mean_and_cov([[0,1],[0,3.5]],[[[1, .8],[.8, 1]],[[1, .8],[.8, 1]]],[1,0],mode='test'),\n",
        "    'test4':gen_from_mean_and_cov([[1,0],[3,1]],[[[1, .8],[.8, 1]],[[1, .9],[.9, 1]]],[1,0],mode='test'),\n",
        "    'test5':gen_from_mean_and_cov([[0,0],[4,0],[-4,0],[0,4],[0,-4]],[[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]]],[1,0,0,0,0],mode='test'),\n",
        "    'test6':gen_from_mean_and_cov(\n",
        "    [[0,0],[4,0],[-4,0],[0,4],[0,-4],[-4,4],[4,4],[4,-4],[-4,-4]],\n",
        "    [[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]]],\n",
        "    [1,0,0,0,0,1,1,1,1],mode='test')\n",
        "}\n",
        "\n",
        "# Print the first dataset:\n",
        "dataset = datasets['train1']\n",
        "print(dataset.head())  # head just gets the first 5 rows\n",
        "\n",
        "\n",
        "# if you want to write out the CSV and re-read it\n",
        "# df.to_csv('tmp.csv')\n",
        "# df = pd.read_csv('tmp.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp3Kof8UIoAQ",
        "colab_type": "text"
      },
      "source": [
        "### Visualize Data\n",
        "Below is a reprentation of one of our datasets. Our task is to figure out how to classify a new point (red or blue) given its coordinates. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TpCzV7iPjxN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(dataset.x1,dataset.x2,c=dataset.y,cmap=plt.get_cmap('jet'));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC4SuoTFTOe3",
        "colab_type": "text"
      },
      "source": [
        "## Method 0: Random Forest\n",
        "This method will be demonstrated, however you will need to complete the following methods yourself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv3jXerVTjrR",
        "colab_type": "text"
      },
      "source": [
        "### Generate Classifiers and Fit the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1QTxwGmmGwM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This code block shows some basic steps for how to access data and load it into a model\n",
        "\n",
        "# To get data, you can select the training set you want (train1 - train6):\n",
        "df = datasets['train1']\n",
        "\n",
        "# Then we have to pull the data out of the dataframe and into a format our model can understand.\n",
        "# In this line, we get two lists from the dataframe which have to converted to numpy arrays. \n",
        "# We then \"stack\" them on top of eachother across the second axis to create a list of coordinate \n",
        "# pairs. X will now be a list of shape (N, 2) -> N coordinate pairs. \n",
        "\n",
        "X = np.stack((df.x1.to_numpy(), df.x2.to_numpy()), axis=1)\n",
        "\n",
        "# Uncoment these lines to see how X is structured:\n",
        "# print(X.shape)\n",
        "# print(X)\n",
        "\n",
        "# The feature we want to train will be called 'Y'\n",
        "Y = df.y.to_numpy()\n",
        "\n",
        "# Next we create the classifier we want to construct:\n",
        "my_classifier = RandomForestClassifier(n_estimators=100, max_depth=6)\n",
        "\n",
        "# Then we \"fit\" the data to the model.\n",
        "my_classifier.fit(X, Y)\n",
        "\n",
        "# Our model has now seen a sample of some probability distribution `D` (the distribution of all \n",
        "# possible (X,Y) that might appear in a dataset like this one), and can now estimate future (x,y) pairs.\n",
        "\n",
        "# Get the testing set that corresponds with our original training set\n",
        "df_test = datasets['test1']\n",
        "\n",
        "# We have to structure our data in the same way that we did before!\n",
        "X = np.stack((df_test.x1.to_numpy(), df_test.x2.to_numpy()), axis=1)\n",
        "Y = df.y.to_numpy()\n",
        "\n",
        "# Now we can use our fitted model on these data points and predict what label Y they might have:\n",
        "predictions = my_classifier.predict(X)\n",
        "\n",
        "# Predictions is now a list that contains what label (0 or 1) we believe corresponds to each point in\n",
        "# the test set. Y contains the \"ground truth\" labels. We can compare these lists to see how well our\n",
        "# model performed. Visually, we can construct two graphs. One with the predictions and one with the\n",
        "# ground truths. If they are close to being the same (or are the same), then our model did well. \n",
        "\n",
        "print(\"Predictions\")\n",
        "plt.scatter(df_test.x1,df_test.x2,c=predictions,edgecolors='w',cmap=plt.get_cmap('jet'))\n",
        "plt.show()\n",
        "\n",
        "print(\"Ground Truth\")\n",
        "plt.scatter(df_test.x1,df_test.x2,c=Y,edgecolors='w',cmap=plt.get_cmap('jet'))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRwQg-1dqGqJ",
        "colab_type": "text"
      },
      "source": [
        "As you should see after running the code, both charts are completely identical. This means our model was successful. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfat16q-TizU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In this code block, we will create a Random Forest classifier for each dataset. Then, \n",
        "# we will use each classifier to create decision boundaries on our dataset.\n",
        "rf_fitted_models = []\n",
        "\n",
        "for ds, df in datasets.items():\n",
        "  # Only train on the training dataset!\n",
        "  if \"train\" in ds:\n",
        "    # Create classifier\n",
        "    new_rf = RandomForestClassifier(n_estimators=100, max_depth=6)\n",
        "    \n",
        "    # In this section, we \"stack\" the data in order to create (feature, target) pairs.\n",
        "    # This is the structure required to \"fit\" our data to our model. \n",
        "    X = np.stack((df.x1.to_numpy(), df.x2.to_numpy()), axis=1)\n",
        "    Y = df.y.to_numpy()\n",
        "    new_rf.fit(X, Y)\n",
        "\n",
        "    rf_fitted_models.append(new_rf)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akrvxihyT9zA",
        "colab_type": "text"
      },
      "source": [
        "### Visualize Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Be-w4vHUC0M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This block uses code to iterate over all the training test sets. It will generate a chart that shows\n",
        "# the true label for each point in the test set, but also the confidence of our model in classifying any\n",
        "# point that might be tested. It will also create a decision boundary.\n",
        "# The colorbar shows how confident we are in a given label for a given location.\n",
        "\n",
        "for ds, df in datasets.items():\n",
        "    if \"test\" in ds:\n",
        "        idx = int(ds[4]) -1\n",
        "        clf = rf_fitted_models[idx]        \n",
        "        visualize_classifier(df, clf)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pznwasy5bJEy",
        "colab_type": "text"
      },
      "source": [
        "Notice how logistic regression has a hard time being sure of the last diagram. The bottom graph is the same as the middle graph but with the scatterplot turned off. Even at the dead center, it is just slightly better than randomly guessing whether that should be red or blue. The models below are much stronger at handling non-linear relationships, as you will soon see."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoHb1rnFeXjq",
        "colab_type": "text"
      },
      "source": [
        "## Method 1: k-Nearest Neighbors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIW8MTsn2gcK",
        "colab_type": "text"
      },
      "source": [
        "### Generate Classifiers and Fit the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ne_jXEljbgMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TO DO\n",
        "# Generate classifiers for each synthetic dataset (6 classifiers in total) using k-Nearest Neighbors. Make \n",
        "# sure you are training on the training set and calling the visualization function on the test set.\n",
        "# Note: use the sklearn function! Try playing around with different values for n_neighbors.\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOrvOu5XDLJz",
        "colab_type": "text"
      },
      "source": [
        "### Visualize Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUa1lBvulgzW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TO DO\n",
        "# Generate charts from your classifiers and the testing dataset. There should be 6 charts!\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NZqPLFJWlI6",
        "colab_type": "text"
      },
      "source": [
        "## Method 2: Decision Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z9M4NFIWsQ-",
        "colab_type": "text"
      },
      "source": [
        "### Generate Classifiers and Fit the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boSYZbYrZagl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TO DO\n",
        "# Generate classifiers for each synthetic dataset (6 classifiers in total) using Decision Trees. Make \n",
        "# sure you are training on the training set and calling the visualization function on the test set.\n",
        "# Note: use the sklearn function! Try playing around with different values max_depth.\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-AwuqgfWvF-",
        "colab_type": "text"
      },
      "source": [
        "### Visualize Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFWyFzeFZpPA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TO DO\n",
        "# Generate charts from your classifiers and the testing dataset. There should be 6 charts!\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgsukuz7Z2dH",
        "colab_type": "text"
      },
      "source": [
        "## Method 3: Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lMq86wSaCxL",
        "colab_type": "text"
      },
      "source": [
        "### Generate Classifiers and Fit the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5MTH4jaaPDs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TO DO\n",
        "# Generate classifiers for each synthetic dataset (6 classifiers in total) using Logisitic Regression.\n",
        "# Make sure you are training on the training set and calling the visualization function on the test set.\n",
        "# Note: use the sklearn function! \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quS26XZ8aEAh",
        "colab_type": "text"
      },
      "source": [
        "### Visualize Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SYkrZuRaUjI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TO DO\n",
        "# Generate charts from your classifiers and the testing dataset. There should be 6 charts!\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}